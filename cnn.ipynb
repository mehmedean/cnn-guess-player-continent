{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3457)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitFilename(filename):\n",
    "    return [word.replace('-', ' ') for word in filename.split(separator)[:-1]]\n",
    "\n",
    "def random_batch(x, y):\n",
    "    n_data = len(x)\n",
    "\n",
    "    # Create a random index.\n",
    "    idx = np.random.choice(n_data,\n",
    "                           size=batch_size,\n",
    "                           replace=False)\n",
    "\n",
    "    # Use the random index to select random images and labels.\n",
    "    x_batch = x[idx, :, :, :]\n",
    "    y_batch = y[idx, :]\n",
    "\n",
    "    yield (x_batch, y_batch)\n",
    "\n",
    "def getbatch(x, y, batch_size=256):\n",
    "    n_batches = n_data//batch_size\n",
    "    for batch_number in range(n_batches):\n",
    "        rand_index = np.array([random.randrange(0, n_data) for i in range(batch_size)])\n",
    "#         print(rand_index)\n",
    "        batch_x = x[rand_index, :, :, :]\n",
    "#         print(\"x.shape:\", batch_x.shape)\n",
    "#         batch_x = (batch_x-batch_x.min())/(batch_x.max()-batch_x.min())\n",
    "        batch_y = y[rand_index]\n",
    "#         print(\"y.shape:\", batch_y.shape)\n",
    "        yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Global constants\n",
    "images_directory = './images_3/'\n",
    "separator = \"_\"\n",
    "image_width = 136\n",
    "image_height = 180\n",
    "image_channel = 3\n",
    "image_shape = (image_height, image_width, image_channel)\n",
    "image_size = image_height * image_width * image_channel\n",
    "continents = {'africa': 0, 'asia': 1, 'australia': 2, 'europe': 3, 'north america': 4, 'south america': 5}\n",
    "country_continent_dict = {'morocco': 'africa', 'kosovo': 'europe', 'guinea bissau': 'africa', 'australia': 'australia', 'kazakhstan': 'asia', 'montenegro': 'europe', 'sao tome and principe': 'africa', 'madagascar': 'africa', 'latvia': 'europe', 'tunisia': 'africa', 'spain': 'europe', 'japan': 'asia', 'italy': 'europe', 'mali': 'africa', 'peru': 'south america', 'brazil': 'south america', 'martinique': 'north america', 'togo': 'africa', 'bolivia': 'south america', 'macedonia': 'europe', 'lithuania': 'europe', 'comoros': 'africa', 'senegal': 'africa', 'turkey': 'europe', 'venezuela': 'south america', 'neukaledonien': 'australia', 'sierra leone': 'africa', 'kyrgyzstan': 'asia', 'northern ireland': 'europe', 'dominican republic': 'north america', 'korea, north': 'asia', 'panama': 'north america', 'canada': 'north america', 'mauritania': 'africa', 'central african republic': 'africa', 'poland': 'europe', 'french guiana': 'africa', 'afghanistan': 'asia', 'armenia': 'asia', 'eritrea': 'africa', 'cyprus': 'europe', 'switzerland': 'europe', 'kenya': 'africa', 'faroe island': 'europe', 'georgia': 'asia', 'iran': 'asia', 'korea, south': 'asia', 'saudi arabia': 'asia', 'finland': 'europe', 'guadeloupe': 'north america', 'estonia': 'europe', 'serbia': 'europe', 'denmark': 'europe', 'greece': 'europe', 'chile': 'south america', 'liechtenstein': 'europe', \"cote d'ivoire\": 'africa', 'cape verde': 'africa', 'portugal': 'africa', 'england': 'europe', 'azerbaijan': 'asia', 'dr congo': 'africa', 'sweden': 'europe', 'rwanda': 'africa', 'zambia': 'africa', 'iraq': 'asia', 'egypt': 'africa', 'hungary': 'europe', 'honduras': 'north america', 'equatorial guinea': 'africa', 'mexico': 'north america', 'slovenia': 'europe', 'france': 'europe', 'jamaica': 'north america', 'argentina': 'south america', 'uruguay': 'south america', 'mauritius': 'africa', 'chad': 'africa', 'benin': 'africa', 'nigeria': 'africa', 'uzbekistan': 'asia', 'trinidad and tobago': 'africa', 'ukraine': 'europe', 'ireland': 'europe', 'algeria': 'africa', 'czech republic': 'europe', 'paraguay': 'south america', 'united states': 'north america', 'romania': 'europe', 'iceland': 'europe', 'austria': 'europe', 'bulgaria': 'europe', 'gabon': 'africa', 'ecuador': 'south america', 'russia': 'europe', 'uganda': 'africa', 'china': 'asia', 'albania': 'europe', 'cameroon': 'africa', 'pal√§stina': 'asia', 'guinea': 'africa', 'scotland': 'europe', 'slovakia': 'europe', 'belarus': 'europe', 'luxembourg': 'europe', 'belgium': 'europe', 'germany': 'europe', 'israel': 'asia', 'lebanon': 'asia', 'indonesia': 'asia', 'malta': 'europe', 'mozambique': 'africa', 'south africa': 'africa', 'congo': 'africa', 'new zealand': 'australia', 'wales': 'europe', 'tanzania': 'africa', 'curacao': 'south america', 'costa rica': 'north america', 'haiti': 'north america', 'jordan': 'asia', 'libya': 'africa', 'philippines': 'asia', 'croatia': 'europe', 'angola': 'africa', 'bosnia herzegovina': 'europe', 'tajikistan': 'asia', 'chinese taipei (taiwan)': 'asia', 'colombia': 'south america', 'netherlands': 'europe', 'burundi': 'africa', 'liberia': 'africa', 'ghana': 'africa', 'norway': 'europe', 'moldova': 'europe', 'syria': 'asia', 'zimbabwe': 'africa', 'the gambia': 'africa', 'burkina faso': 'africa'}\n",
    "\n",
    "n_data = 0\n",
    "for filename in os.listdir(images_directory):\n",
    "    if os.path.isfile(os.path.join(images_directory, filename)) and filename.endswith(\".jpg\"):\n",
    "        image = Image.open(images_directory + filename)\n",
    "        im2arr = np.array(image) # im2arr.shape: height x width x channel\n",
    "        if im2arr.shape[0] != image_height or im2arr.shape[1] != image_width or im2arr.shape[2] != image_channel:\n",
    "            continue\n",
    "        n_data += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "learningrate = 0.001\n",
    "\n",
    "# n_data = len([name for name in os.listdir(images_directory)\n",
    "#               if os.path.isfile(os.path.join(images_directory, name))\n",
    "#               and name.endswith(\".jpg\")])\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "# n_batches = 10 # n_data//batch_size\n",
    "n_input = image_size\n",
    "n_classes = 6\n",
    "dropout = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.]\n",
      "Data Size: 7493\n",
      "Index: 7493\n",
      "(7493, 180, 136, 3)\n",
      "(7493, 6)\n",
      "[ 1.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.ndarray(shape=(n_data, image_height, image_width, image_channel), dtype=np.float32)\n",
    "y = []\n",
    "\n",
    "index = 0\n",
    "for filename in os.listdir(images_directory):\n",
    "    if os.path.isfile(os.path.join(images_directory, filename)) and filename.endswith(\".jpg\"):\n",
    "        # print(filename)\n",
    "        image = Image.open(images_directory + filename)\n",
    "        im2arr = np.array(image) # im2arr.shape: height x width x channel\n",
    "        if im2arr.shape[0] != image_height or im2arr.shape[1] != image_width or im2arr.shape[2] != image_channel:\n",
    "            continue\n",
    "        x[index] = Image.fromarray(np.array(image))\n",
    "        y.append(continents[country_continent_dict[splitFilename(filename)[3]]])\n",
    "        index += 1\n",
    "\n",
    "# One-hot encode labels for all data\n",
    "y = np.array(y, dtype=np.int)\n",
    "y = np.eye(n_classes)[y]\n",
    "print(y[25])\n",
    "\n",
    "print(\"Data Size:\", n_data)\n",
    "print(\"Index:\", index)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(y[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining X and Y as placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 180, 136, 3) (?, 6)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, image_height, image_width, image_channel])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model Using conv2d, ReLu and Maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Convolution layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max pooling\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 3, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([34*45*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Cost, Optimizer and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_1:0\", shape=(?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = conv_net(X, weights, biases, keep_prob)\n",
    "print(model)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "train_min = tf.train.AdamOptimizer(learning_rate=learningrate).minimize(loss)\n",
    "\n",
    "# Evaluate model\n",
    "correct_model = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_model, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 27244652.0000, Accuracy: 0.7344\n",
      "Epoch: 1, Loss: 16264012.0000, Accuracy: 0.7227\n",
      "Epoch: 2, Loss: 7355523.0000, Accuracy: 0.6914\n",
      "Epoch: 3, Loss: 6907926.0000, Accuracy: 0.7383\n",
      "Epoch: 4, Loss: 3421578.5000, Accuracy: 0.7266\n",
      "Epoch: 5, Loss: 2291593.7500, Accuracy: 0.7578\n",
      "Epoch: 6, Loss: 1243703.8750, Accuracy: 0.8008\n",
      "Epoch: 7, Loss: 888712.0000, Accuracy: 0.7891\n",
      "Epoch: 8, Loss: 463129.4375, Accuracy: 0.7773\n",
      "Epoch: 9, Loss: 255529.4375, Accuracy: 0.7383\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e54aa7e4b6b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# When the training is complete and you are happy with the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     accuracycalc = sess.run(accuracy,\n\u001b[0;32m---> 28\u001b[0;31m                             feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1.0})\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing accuracy: %0.4f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracycalc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_x, batch_y in getbatch(x, y):\n",
    "            sess.run(train_min, feed_dict={X:batch_x, Y:batch_y, keep_prob: dropout})\n",
    "        for batch_x, batch_y in getbatch(x, y):\n",
    "            losscalc, accuracycalc = sess.run([loss, accuracy], \n",
    "                                              feed_dict={X:batch_x, Y:batch_y, keep_prob: 1.0})\n",
    "            print(\"Epoch: %d, Loss: %0.4f, Accuracy: %0.4f\" % (epoch, losscalc, accuracycalc))\n",
    "            break;\n",
    "            \n",
    "#         for _ in range(n_batches):\n",
    "#             got = getbatch(x, y)\n",
    "#             print(\"got:\", got)\n",
    "#             batch_x, batch_y = getbatch(x, y)\n",
    "#             #batch_y = batch_y.astype(np.float32)\n",
    "#             # Use training data for optimization\n",
    "#             sess.run(train_min, feed_dict={X:batch_x, Y:batch_y, keep_prob: dropout})\n",
    "        # Validate after every epoch\n",
    "#         batch_x, batch_y = getbatch(x, y)\n",
    "#         losscalc, accuracycalc = sess.run([loss, accuracy], \n",
    "#                                           feed_dict={X:batch_x, Y:batch_y, keep_prob: 1.0})\n",
    "#         print(\"Epoch: %d, Loss: %0.4f, Accuracy: %0.4f\"%(epoch, losscalc, accuracycalc))\n",
    "            \n",
    "    # When the training is complete and you are happy with the result\n",
    "    accuracycalc = sess.run(accuracy,\n",
    "                            feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1.0})\n",
    "    print(\"Testing accuracy: %0.4f\"%(accuracycalc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 26114432.0000, Accuracy: 0.7070\n",
      "Epoch: 1, Loss: 13132954.0000, Accuracy: 0.7383\n",
      "Epoch: 2, Loss: 8819888.0000, Accuracy: 0.6992\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7750ba23df23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             losscalc, accuracycalc = sess.run([loss, accuracy], \n",
      "\u001b[0;32m/anaconda/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tf-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_x, batch_y in getbatch(x, y):\n",
    "            sess.run(train_min, feed_dict={X:batch_x, Y:batch_y, keep_prob: dropout})\n",
    "        for batch_x, batch_y in getbatch(x, y):\n",
    "            losscalc, accuracycalc = sess.run([loss, accuracy], \n",
    "                                              feed_dict={X:batch_x, Y:batch_y, keep_prob: 1.0})\n",
    "            print(\"Epoch: %d, Loss: %0.4f, Accuracy: %0.4f\" % (epoch, losscalc, accuracycalc))\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
